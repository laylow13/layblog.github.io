[{"title":"神经网络与深度学习课程总结一","url":"/2024/04/01/神经网络与深度学习课程总结一/","content":"### 线性回归\n\n- **定义与基本概念**：线性回归用于确定变量间相互依赖的定量关系，是一种统计分析方法。以房屋面积与销售价格的关系为例，通过拟合一条直线（模型）来预测未知面积的房屋价格。\n- **数学模型**：模型表示为 $y = h_{\\theta}(x) = \\theta^Tx + \\theta_0$，其中 $x$ 和 $y$ 分别是输入和输出，$\\theta$ 表示模型参数。\n- **代价函数**：$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2$，目标是最小化代价函数。\n- **解析解**：通过求解 $\\nabla_{\\theta} J(\\theta) = 0$ 得到参数的解析解 $\\theta = (X^TX)^{-1}X^Ty$。\n\n### 线性二分类问题\n\n- **基本概念**：线性分类通过特征的线性组合进行分类决策，例如通过直线或超平面分割不同类别的样本。\n- **Sigmoid函数**：用于将线性函数的输出转换为概率值，形式为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- **代价函数**：与线性回归相似，但引入了Sigmoid函数处理分类问题。\n\n### 对数回归与多分类回归\n\n- **对数回归**：使用Sigmoid函数处理二分类问题，输出为{0,1}。代价函数采用交叉熵损失。\n- **多分类回归（Softmax回归）**：处理多类别的分类问题，使用Softmax函数将线性函数的输出转换为各类别的概率分布。\n\n### 神经元模型\n\n- **M-P模型**：1943年由McCulloch和Pitts提出的神经元模型，是神经网络的基础。\n- **作用函数**：介绍了非对称型Sigmoid函数和对称型Sigmoid函数等，用于模拟神经元的激活过程。\n\n### 感知机模型\n\n- **原理与模型**：感知机是一种简单的线性二分类模型，通过迭代优化模型参数来分割不同类别的样本。\n- **训练过程**：感知机通过迭代调整权重，直到找到能够正确分类所有训练样本的超平面。\n\n### 多层感知机\n\n- 多层感知机解决了单层网络（如感知机）无法解决的线性不可分问题，例如XOR问题。\n- 通过引入至少一层隐藏层，多层感知机可以实现任意复杂度的函数逼近。\n- 数学表述：\n  - 激活函数用于隐层节点，如Sigmoid函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ ，使网络能逼近非线性函数。\n  - 输出表达式：$ y = \\sigma(w_2 \\sigma(w_1 x + b_1) + b_2)$ ，其中$x$是输入，$y$是输出，$w_1, w_2$是权重，$b_1, b_2$是偏置项。\n\n### BP算法\n\n- BP算法是一种训练多层前馈神经网络的方法，通过正向传播输入信号，并通过反向传播误差信号来调整权值和阈值。\n- **正向传播**：输入信号从输入层经过隐藏层传递到输出层，如果输出层的输出与期望的输出相符，则结束学习过程；否则，进入反向传播。\n- **反向传播**：计算输出与实际值之间的误差，并将误差沿网络反向传播，利用梯度下降法更新每层的权重和偏置，以减小网络的预测误差。\n- 数学表述：\n  - 误差函数：$ E = \\frac{1}{2} \\sum (y_{actual} - y_{predicted})^2$ ，其中$E$表示网络的总误差。\n  - 权重更新：$\\Delta w = -\\eta \\frac{\\partial E}{\\partial w}$ ，$\\eta$是学习率，$\\Delta w$是权重的调整量。\n\n### 算法优缺点\n\n- **优点**：可以自主学习，逼近任意非线性函数。\n- **缺点**：算法可能不会全局收敛，收敛速度可能慢，需要合理选择学习率，神经网络的设计（如层数和每层的节点数）具有挑战性。","tags":["课程"]}]