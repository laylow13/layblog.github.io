[{"title":"神经网络与深度学习课程总结二","url":"/2024/04/16/神经网络与深度学习课程总结二/","content":"\n### 全连接网络的限制\n\n- 全连接层面临参数过多，计算缓慢和容易过拟合的问题。\n\n- 通过局部连接和参数共享，卷积网络显著减少了参数数量，提高了计算效率。\n\n### 卷积神经网络\n\n#### 基本组件\n\n1. **卷积层 (Convolutional Layer)**\n   \n   - **功能**：主要用于特征提取。通过卷积操作，使用多个过滤器（卷积核）从输入图像或前一层的特征图中提取特征。\n   - **操作**：每个过滤器在输入数据上滑动，计算过滤器与输入数据的点积，生成特征图（Feature Map）。\n\n2. **激活函数 (Activation Function)**\n   \n   - **常用激活函数**：ReLU (Rectified Linear Unit) 是最常用的，用于增加网络的非线性，没有它网络就无法学习复杂的数据模式。\n   - **功能**：在每个卷积层后应用，用于引入非线性，帮助网络学习更复杂的特征。\n\n3. **池化层 (Pooling Layer)**\n   \n   - **类型**：最大池化（Max Pooling）和平均池化（Average Pooling）是最常见的。\n   - **功能**：减少特征图的维度，提高计算效率，同时使特征检测变得对输入的小变化更加鲁棒。\n\n4. **全连接层 (Fully Connected Layer)**\n   \n   - **功能**：在卷积层和池化层提取并缩减了特征后，全连接层用于对这些特征进行分类或回归分析。\n   - **结构**：每个神经元都与前一层的所有神经元连接，其输出通过权重求和，通常在网络的末尾使用，用于输出最终的预测结果。\n\n#### 基本操作\n\n1. **填充 (Padding)**\n   \n   - **目的**：在卷积操作前，为输入数据的边缘添加额外的层（通常是0），以保持数据的空间尺寸，或控制特征图的缩小速度。\n\n2. **步长 (Stride)**\n   \n   - **定义**：过滤器在输入数据上移动的步长。步长大于1可以减少输出的空间维度，增加步长会减小特征图的尺寸。\n\n3. **多通道卷积**\n   \n   - **概念**：现代CNN通常处理RGB三通道的彩色图像。卷积层能够处理多通道输入，每个通道有自己的权重，最后将这些通道的结果相加得到输出特征图。\n\n#### 典型的CNN架构\n\n- **输入层**：接收原始图像数据。\n- **隐藏层**：\n  - 多个卷积层，每个后跟一个激活函数。\n  - 池化层间歇地插入在卷积层之间以减少维度。\n- **全连接层**：在多个卷积和池化层之后，通常会有几层全连接层，最终输出预测结果。\n- **输出层**：输出最终的分类或回归结果。\n\n这些组成部分在卷积神经网络中相互作用，使得CNN特别适用于图像和视频处理任务，因为它们可以有效地捕获空间和时间的层次结构特征。\n\n### LeNet-5 网络结构\n\n1. **C1卷积层**：\n   \n   - 该层使用6个大小为5x5的过滤器（卷积核），进行卷积操作。\n   - 输出特征图（Feature Map）的维度为28x28，每个Feature Map由一个5x5的局部区域通过卷积生成。\n   - 总共有6个Feature Map，因此这一层有(5x5+1)x6 = 156个参数（+1是因为偏置项）。\n\n2. **S2池化层**（下采样层）：\n   \n   - 采用2x2的区域进行平均池化，步长为2，用于降低特征维度和减少计算复杂性。\n   - 输出为14x14x6的特征图（每个Feature Map大小为14x14，共6个Feature Map）。\n\n3. **C3卷积层**：\n   \n   - 使用16个不同的5x5过滤器对S2层的输出进行卷积。\n   - 不是每个C3过滤器都与S2的所有6个Feature Maps连接，这种部分连接的结构有助于网络提取更丰富的特征。\n   - 输出为10x10x16的特征图。\n\n4. **S4池化层**：\n   \n   - 与S2相似，使用2x2平均池化，输出为5x5x16的特征图。\n\n5. **C5卷积层**：\n   \n   - 这层通常被视为一个全连接层，因为其过滤器的大小与输入相同（5x5），直接对每个完整的Feature Map进行卷积。\n   - 输出为1x1x120的特征图。\n\n6. **全连接层**：\n   \n   - 第一个全连接层有84个神经元，每个神经元连接到C5层的所有120个输出。\n   - 第二个全连接层通常是输出层，根据具体任务可能有不同数量的神经元（例如，数字识别为10个类别）。\n\n### AlexNet\n\n- 包含五个卷积层和三个全连接层。\n- 使用ReLU作为激活函数，有效解决了Sigmoid在深层网络中容易引起的梯度消失问题。\n- 引入了局部响应归一化（LRN），提高了模型的泛化能力。\n- 使用重叠的最大池化，减少了特征图的尺寸，同时保持了重要的特征。\n- 实现了多GPU训练，大大加速了网络的训练过程。\n- 采用Dropout减少过拟合。\n\n### VGG-16\n\n- 由16层权重层组成，包括13个卷积层和3个全连接层。\n- 所有隐藏层都使用相同的卷积核大小（3x3），池化层使用2x2的最大池化。\n- 结构规整，每通过一层池化层后，卷积层的深度加倍。\n- 由于其深度和数百万的参数，VGG-16是一个训练和推理计算成本都很高的模型。\n\n### 残差网络（ResNet）\n\n- 引入残差模块，每个残差模块包含两个或三个卷积层，以及一个跳跃连接，后者允许梯度直接传递。\n\n- 解决了更深网络中的梯度消失和爆炸问题，使得网络可以扩展到数十甚至上百层。\n\n- 性能优越，极大地推动了深度学习在许多领域的应用，包括图像分类、目标检测和语义分割。\n\n### YOLO\n\n#### 主要特点\n\n1. **速度快**：YOLO可以达到实时的目标检测速度，非常适合需要实时反应的应用场景，如视频内容分析和自动驾驶。\n\n2. **在一个网络中处理**：传统的目标检测系统设计复杂，需要多个阶段来处理图像。而YOLO将整个目标检测流程整合到一个单一的网络中，这使得它既快速又简洁。\n\n3. **泛化能力强**：虽然YOLO在处理小对象时可能存在局限性，但其在不同背景下对于大对象的检测显示出较强的泛化能力。\n\n#### 工作原理\n\nYOLO将输入图像分割成一个SxS的网格，并对每个网格预测B个边界框，每个边界框包括边界框的坐标、置信度和类别概率。置信度反映了模型对边界框包含目标的确信程度以及预测边界框的准确性。类别概率表示边界框中物体属于某一类别的概率。\n\n#### 网络架构\n\nYOLO的网络架构由多个卷积层和全连接层组成，它使用单个卷积神经网络直接预测多个边界框及其置信度和类别概率。\n\n\n","tags":["课程"]},{"title":"神经网络与深度学习课程总结一","url":"/2024/04/01/神经网络与深度学习课程总结一/","content":"### 线性回归\n\n- **定义与基本概念**：线性回归用于确定变量间相互依赖的定量关系，是一种统计分析方法。以房屋面积与销售价格的关系为例，通过拟合一条直线（模型）来预测未知面积的房屋价格。\n- **数学模型**：模型表示为 $y = h_{\\theta}(x) = \\theta^Tx + \\theta_0$，其中 $x$ 和 $y$ 分别是输入和输出，$\\theta$ 表示模型参数。\n- **代价函数**：$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2$，目标是最小化代价函数。\n- **解析解**：通过求解 $\\nabla_{\\theta} J(\\theta) = 0$ 得到参数的解析解 $\\theta = (X^TX)^{-1}X^Ty$。\n\n### 线性二分类问题\n\n- **基本概念**：线性分类通过特征的线性组合进行分类决策，例如通过直线或超平面分割不同类别的样本。\n- **Sigmoid函数**：用于将线性函数的输出转换为概率值，形式为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- **代价函数**：与线性回归相似，但引入了Sigmoid函数处理分类问题。\n\n### 对数回归与多分类回归\n\n- **对数回归**：使用Sigmoid函数处理二分类问题，输出为{0,1}。代价函数采用交叉熵损失。\n- **多分类回归（Softmax回归）**：处理多类别的分类问题，使用Softmax函数将线性函数的输出转换为各类别的概率分布。\n\n### 神经元模型\n\n- **M-P模型**：1943年由McCulloch和Pitts提出的神经元模型，是神经网络的基础。\n- **作用函数**：介绍了非对称型Sigmoid函数和对称型Sigmoid函数等，用于模拟神经元的激活过程。\n\n### 感知机模型\n\n- **原理与模型**：感知机是一种简单的线性二分类模型，通过迭代优化模型参数来分割不同类别的样本。\n- **训练过程**：感知机通过迭代调整权重，直到找到能够正确分类所有训练样本的超平面。\n\n### 多层感知机\n\n- 多层感知机解决了单层网络（如感知机）无法解决的线性不可分问题，例如XOR问题。\n- 通过引入至少一层隐藏层，多层感知机可以实现任意复杂度的函数逼近。\n- 数学表述：\n  - 激活函数用于隐层节点，如Sigmoid函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ ，使网络能逼近非线性函数。\n  - 输出表达式：$y = \\sigma(w_2 \\sigma(w_1 x + b_1) + b_2)$ ，其中$x$是输入，$y$是输出，$w_1, w_2$是权重，$b_1, b_2$是偏置项。\n\n### BP算法\n\n- BP算法是一种训练多层前馈神经网络的方法，通过正向传播输入信号，并通过反向传播误差信号来调整权值和阈值。\n- **正向传播**：输入信号从输入层经过隐藏层传递到输出层，如果输出层的输出与期望的输出相符，则结束学习过程；否则，进入反向传播。\n- **反向传播**：计算输出与实际值之间的误差，并将误差沿网络反向传播，利用梯度下降法更新每层的权重和偏置，以减小网络的预测误差。\n- 数学表述：\n  - 误差函数：$E = \\frac{1}{2} \\sum (y_{actual} - y_{predicted})^2$ ，其中$E$表示网络的总误差。\n  - 权重更新：$\\Delta w = -\\eta \\frac{\\partial E}{\\partial w}$ ，$\\eta$是学习率，$\\Delta w$是权重的调整量。\n\n### 算法优缺点\n\n- **优点**：可以自主学习，逼近任意非线性函数。\n- **缺点**：算法可能不会全局收敛，收敛速度可能慢，需要合理选择学习率，神经网络的设计（如层数和每层的节点数）具有挑战性。","tags":["课程"]}]